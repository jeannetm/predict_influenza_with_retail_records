{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import csv\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the year of the study\n",
    "year = 2010"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We find peak and period of interest from flu time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flu_series = np.genfromtxt(\"../Data/Influenza/%d-%d_Italy.txt\" %(year, year + 1), skiprows=1, dtype=None, usecols= 1)\n",
    "sum_flu = sum(flu_series)\n",
    "normal_flu_series = [x / sum_flu for x in flu_series]  # Series normalization\n",
    "\n",
    "influenza_peak = max(flu_series) # Flu peak\n",
    "influenza_index = list(flu_series).index(influenza_peak)\n",
    "\n",
    "if influenza_index <= 10: # We match the index with the actual week of the year (reporting starts at week 42)\n",
    "    week_peak = influenza_index + 42\n",
    "else:\n",
    "    week_peak = influenza_index - 10\n",
    "\n",
    "window = [week_peak - 2, week_peak - 1, week_peak, week_peak + 1, week_peak + 2] # Window of interest -> peak +- 2 weeks\n",
    "window_list = [str(x) for x in window]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We find the products of interest - sentinel products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = []\n",
    "correlations = []\n",
    "normal_product_series_dic = {}\n",
    "\n",
    "# subcategory,value_week1, value_week2, ...\n",
    "f_coop_weeks = gzip.open(\"../Data/COOP/coop_flu_weeks_%d_%d.csv.gz\" %(year, year + 1))\n",
    "file_reader = csv.reader(f_coop_weeks)\n",
    "for line in file_reader:\n",
    "    product = line[0]\n",
    "    products.append(product)\n",
    "    line.remove(product)\n",
    "    product_series = map(float, line)\n",
    "    product_series = np.array(product_series)\n",
    "    sum_product = sum(product_series)\n",
    "    normal_product_series = [x / sum_product for x in product_series]  # Series normalization\n",
    "    normal_product_series_dic[product] = normal_product_series # We keep the time series of each product\n",
    "    correlations.append(pearsonr(normal_flu_series, normal_product_series)[0])  # Calculate distance between series\n",
    "f_coop_weeks.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original number of products: ' + str(len(correlations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_correlation = [round(float(i), 2) for i in correlations]\n",
    "count = {x: rounded_correlation.count(x) for x in rounded_correlation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = count.values()\n",
    "keys = count.keys()\n",
    "sum_freq = sum(freq)\n",
    "y = [float(x) / sum_freq for x in freq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.scatter(keys, y)\n",
    "plt.title('Correlation distribution %d' %year)\n",
    "plt.ylabel('P(c)')\n",
    "plt.xlabel('Correlation(c)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We notice that there are many products with a negative or zero correlation, so we filter them out setting a threshold of 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the list of final subcategories with positive correlation\n",
    "    \n",
    "flu_products = []\n",
    "\n",
    "for correlation in correlations:\n",
    "    if correlation >= 0.2:\n",
    "        index_correlation = correlations.index(correlation)\n",
    "        flu_products.append(products[index_correlation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of sentinel products: ' + str(len(flu_products)) + ' out of ' + str(len(correlations)) + ' original products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We find the customers of interest - sentinel customers and retrieve all their purchases during the period of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each client is of interest if he/she purchased at least one sentinel product during the period of interest\n",
    "\n",
    "clients = []\n",
    "\n",
    "# client,year,week,receipt,subcategory,quantity\n",
    "f_coop_receipts = gzip.open(\"../Data/COOP/coop_flu_receipts_%d_%d.csv.gz\" %(year, year + 1))\n",
    "file_reader = csv.DictReader(f_coop_receipts, delimiter=',')\n",
    "for line in file_reader:\n",
    "    product = line['subcategory']\n",
    "    week = line['week']\n",
    "    if product in flu_products:\n",
    "        if week in window_list: # period of interest\n",
    "            clients.append(line['client'])\n",
    "f_coop_receipts.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each basket contains only the flu_products from the same receipt\n",
    "\n",
    "receipts = {}\n",
    "\n",
    "f_coop_receipts = gzip.open(\"../Data/COOP/coop_flu_receipts_%d_%d.csv.gz\" %(year, year + 1))\n",
    "products = csv.DictReader(f_coop_receipts, delimiter=',')\n",
    "for line in products:\n",
    "    client = line['client']\n",
    "    receipt = line['receipt']\n",
    "    product = line['subcategory']o\n",
    "    if client in clients:\n",
    "        if product in flu_products:\n",
    "            if receipt not in receipts:\n",
    "                receipts[receipt] = []\n",
    "            receipts[receipt].append(product)            \n",
    "f_coop_receipts.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We find the sentinel baskets with Apriori and we construct their time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a list with all the baskets as an input for the Apriori algorithm\n",
    "baskets = []\n",
    "\n",
    "for receipt in receipts:\n",
    "    baskets.append(receipts[receipt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AprioriAlgorithm import apriori\n",
    "\n",
    "minsupport = 0.01\n",
    "\n",
    "frequent_baskets, support_data = apriori(baskets, minsupport)  # The variable support_data is just a dictionary with the support values of our frequent baskets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series construction for each of the frequent baskets\n",
    "\n",
    "frequent_baskets_series = {}\n",
    "for baskets_set in frequent_baskets:\n",
    "    baskets_set = map(list, baskets_set)\n",
    "    if baskets_set != []:\n",
    "        for basket in baskets_set:\n",
    "            if len(basket) > 1: # We only keep the baskets with more than one products\n",
    "                sum_basket = normal_product_series_dic[basket[0]]\n",
    "                for i in range(1, len(basket)):\n",
    "                    sum_basket = map(lambda a, b : a + b, sum_basket, normal_product_series_dic[basket[i]])\n",
    "                frequent_baskets_series[tuple(basket)] = sum_basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series normalization and correlation\n",
    "\n",
    "normal_frequent_baskets_series = {}\n",
    "correlations_baskets = {}\n",
    "for basket in frequent_baskets_series:\n",
    "    sum_series = sum(frequent_baskets_series[basket])\n",
    "    normal_frequent_baskets_series[basket] = [x / sum_series for x in frequent_baskets_series[basket]] # Series normalization\n",
    "    correlations_baskets[basket] = pearsonr(normal_flu_series, normal_frequent_baskets_series[basket])[0]  # Calculate distance between series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep the top 5 baskets\n",
    "top_5_baskets = dict(sorted([(k,v) for k, v in correlations_baskets.items()], key = lambda x: x[1])[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have the sentinel baskets from last year, we construct the corresponding time series for the next year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_year = year + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We obtain the influenza series\n",
    "\n",
    "next_flu_series = np.genfromtxt(\"../Data/Influenza/%d-%d_Italy.txt\" %(next_year, next_year + 1), skiprows=1, dtype=None, usecols= 1)\n",
    "next_sum_flu = sum(next_flu_series)\n",
    "next_normal_flu_series = [x / next_sum_flu for x in next_flu_series]  # Series normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the sentinel products, we retrieve their time series\n",
    "\n",
    "next_normal_product_series_dic = {}\n",
    "\n",
    "# subcategory,value_week1, value_week2, ...\n",
    "next_f_coop_weeks = gzip.open(\"../Data/COOP/coop_flu_weeks_%d_%d.csv.gz\" %(next_year, next_year + 1))\n",
    "file_reader = csv.reader(next_f_coop_weeks)\n",
    "for line in file_reader:\n",
    "    product = line[0]\n",
    "    if product in flu_products:\n",
    "        line.remove(product)\n",
    "        product_series = map(float, line)\n",
    "        product_series = np.array(product_series)\n",
    "        sum_product = sum(product_series)\n",
    "        normal_product_series = [x / sum_product for x in product_series]  # Series normalization\n",
    "        next_normal_product_series_dic[product] = normal_product_series\n",
    "\n",
    "next_f_coop_weeks.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We construct the time series for the sentinel baskets\n",
    "\n",
    "next_frequent_baskets_series = {}\n",
    "for basket in top_5_baskets.keys():\n",
    "    sum_basket = next_normal_product_series_dic[str(basket[0])]\n",
    "    for i in range(1, len(basket)):\n",
    "        sum_basket = map(lambda a, b : a + b, sum_basket, next_normal_product_series_dic[str(basket[i])])\n",
    "    next_frequent_baskets_series[tuple(basket)] = sum_basket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentinel basket series normalization\n",
    "next_normal_frequent_baskets_series = {}\n",
    "\n",
    "for basket in next_frequent_baskets_series:\n",
    "    sum_series = sum(next_frequent_baskets_series[basket])\n",
    "    if sum_series == 0.0:\n",
    "        next_normal_frequent_baskets_series[basket] = [0.0 for x in next_frequent_baskets_series[basket]]\n",
    "    else:\n",
    "        next_normal_frequent_baskets_series[basket] = [x / sum_series for x in next_frequent_baskets_series[basket]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our procedure is over, so we save our results as an input for the forecast models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File containing each basket and its correlation\n",
    "\n",
    "f_baskets_correlation = open(\"../Data/Sentinels/sentinel_baskets_correlation_%d-%d.csv\" %(next_year, next_year + 1), 'w')\n",
    "\n",
    "for key, value in sorted(top_5_baskets.iteritems(), key=lambda (k,v): (v,k), reverse=True):\n",
    "    f_baskets_correlation.write(\"%s : %s\\n\" % (key, value))\n",
    "f_baskets_correlation.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We obtain the lists with the weeks for this year (i.e. 2010-42, 2010-43, ...) and the next year (i.e. 2011-42, 2011-43, ...)\n",
    "\n",
    "weeks = np.genfromtxt(\"../Data/Influenza/%d-%d_Italy.txt\" %(year, year + 1), skiprows=1, dtype=None, usecols= 0)\n",
    "\n",
    "next_weeks = np.genfromtxt(\"../Data/Influenza/%d-%d_Italy.txt\" %(next_year, next_year + 1), skiprows=1, dtype=None, usecols= 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Files for each of the top_5 sentinel baskets containing their normalized time series (for each week a value) \n",
    "# from the past and also for the next year\n",
    "# week1, value_week1\n",
    "# week2, value_week2\n",
    "# ...\n",
    "\n",
    "to_print = \"\"\n",
    "\n",
    "for basket in top_5_baskets:\n",
    "    f_time_series = open(\"../Data/Sentinels/time_series_of_sentinel_basket_\" + str(basket) + \"_%d-%d.csv\" %(next_year, next_year + 1), 'w')\n",
    "    \n",
    "    f_time_series.write(\"Week TimeSeries\\n\")\n",
    "    \n",
    "    for i in range(0, len(weeks)):\n",
    "        single_week = weeks[i]\n",
    "        month = single_week.split(\"-\")[1]\n",
    "        ye = single_week.split(\"-\")[0]\n",
    "        if month[0] == '0':\n",
    "            month = month.translate(None, '0')\n",
    "            to_print = ye + \"-\" + month\n",
    "        else:\n",
    "            to_print = single_week\n",
    "        f_time_series.write(to_print + \" \" + str(normal_frequent_baskets_series[basket][i]) + \"\\n\")\n",
    "    for i in range(0, len(next_weeks)):\n",
    "        single_week = next_weeks[i]\n",
    "        month = single_week.split(\"-\")[1]\n",
    "        yea = single_week.split(\"-\")[0]\n",
    "        if month[0] == '0':\n",
    "            month = month.translate(None, '0')\n",
    "            to_print = yea + \"-\" + month\n",
    "        else:\n",
    "            to_print = single_week\n",
    "        f_time_series.write(to_print + \" \" + str(next_normal_frequent_baskets_series[basket][i]) + \"\\n\")\n",
    "f_time_series.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
